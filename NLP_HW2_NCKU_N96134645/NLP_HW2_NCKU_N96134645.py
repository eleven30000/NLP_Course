# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QZLiwuz0KpxSZnh3Br_0T4MtuobfBMl_
"""



"""# LSTM-arithmetic

## Dataset
- [Arithmetic dataset](https://drive.google.com/file/d/1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE-/view?usp=sharing)
"""

! pip install seaborn
 ! pip install opencc
 ! pip install -U scikit-learn

import numpy as np
import pandas as pd
import torch
import torch.nn
import torch.nn.utils.rnn
import torch.utils.data
import matplotlib.pyplot as plt
import seaborn as sns
import opencc
import os
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/gdrive')

data_path = "/content/gdrive/MyDrive/python/"

df_train = pd.read_csv(os.path.join(data_path, 'arithmetic_train.csv'))
df_eval = pd.read_csv(os.path.join(data_path, 'arithmetic_eval.csv'))
df_train.head()

# transform the input data to string
df_train['tgt'] = df_train['tgt'].apply(lambda x: str(x))
df_train['src'] = df_train['src'].add(df_train['tgt'])
df_train['len'] = df_train['src'].apply(lambda x: len(x))

df_eval['tgt'] = df_eval['tgt'].apply(lambda x: str(x))
df_eval['src'] = df_eval['src'].add(df_eval['tgt'])
df_eval['len'] = df_eval['src'].apply(lambda x: len(x))

df_train.head()

"""# Build Dictionary
 - The model cannot perform calculations directly with plain text.
 - Convert all text (numbers/symbols) into numerical representations.
 - Special tokens
    - '&lt;pad&gt;'
        - Each sentence within a batch may have different lengths.
        - The length is padded with '&lt;pad&gt;' to match the longest sentence in the batch.
    - '&lt;eos&gt;'
        - Specifies the end of the generated sequence.
        - Without '&lt;eos&gt;', the model will not know when to stop generating.
"""

char_to_id = {}
id_to_char = {}

# write your code here
# Build a dictionary and give every token in the train dataset an id
# The dictionary should contain <eos> and <pad>
# char_to_id is to conver charactors to ids, while id_to_char is the opposite

# Special tokens
special_tokens = ['<pad>', '<eos>']

# 取得 src 欄位中的所有唯一字符
all_chars = set(''.join(df_train['src'])) ##由chatgpt提供寫法##

# 構建 char_to_id 字典
char_to_id = {}
for i, char in enumerate(sorted(all_chars)):
    char_to_id[char] = i

# 加入特殊符號
special_tokens = ['<pad>', '<eos>']
for token in special_tokens:
    char_to_id[token] = len(char_to_id)

# 構建 id_to_char 字典
id_to_char = {id_: char for char, id_ in char_to_id.items()}  ##由chatgpt提供寫法##


vocab_size = len(char_to_id)
print('Vocab size{}'.format(vocab_size))
print(char_to_id)

"""# Data Preprocessing
 - The data is processed into the format required for the model's input and output.
 - Example: 1+2-3=0
     - Model input: 1 + 2 - 3 = 0
     - Model output: / / / / / 0 &lt;eos&gt;  (the '/' can be replaced with &lt;pad&gt;)
     - The key for the model's output is that the model does not need to predict the next character of the previous part. What matters is that once the model sees '=', it should start generating the answer, which is '0'. After generating the answer, it should also generate&lt;eos&gt;

"""

# 刪掉 'Unnamed: 0' 這一列
df_train = df_train.drop(columns=['Unnamed: 0'])
df_eval = df_eval.drop(columns=['Unnamed: 0'])

df_train.head()

# Write your code here

# 將src轉換成對應的id
def convert_to_id_sequence(expression, char_to_id):
    sequence = [char_to_id[char] for char in expression]  # 將每個字符轉成對應的 ID
    sequence.append(char_to_id['<eos>'])  # 在序列結尾加上 <eos>
    return sequence

# 將 `=` 前的字符用 <pad> 的 ID 替換
def create_output_ids(sequence, char_to_id):
    pad_id = char_to_id['<pad>']
    output_ids = sequence.copy()

    # 找到 "=" 的位置
    try:
        equal_index = sequence.index(char_to_id['='])

        # 將 "=" 及其前的字符替換為 <pad> 的 ID
        output_ids[:equal_index + 1] = [pad_id] * (equal_index + 1)
    except ValueError:
        # 若沒有 "="，就不替換
        pass

    return output_ids


# 建立 src_ids 和 output_ids 欄位
df_train['char_id_list'] = df_train['src'].apply(lambda x: convert_to_id_sequence(x, char_to_id))
df_train['label_id_list'] = df_train['char_id_list'].apply(lambda x: create_output_ids(x, char_to_id))

df_eval['char_id_list'] = df_eval['src'].apply(lambda x: convert_to_id_sequence(x, char_to_id))
df_eval['label_id_list'] = df_eval['char_id_list'].apply(lambda x: create_output_ids(x, char_to_id))

df_train.head()
df_eval.head()

"""# Hyper Parameters

|Hyperparameter|Meaning|Value|
|-|-|-|
|`batch_size`|Number of data samples in a single batch|64|
|`epochs`|Total number of epochs to train|10|
|`embed_dim`|Dimension of the word embeddings|256|
|`hidden_dim`|Dimension of the hidden state in each timestep of the LSTM|256|
|`lr`|Learning Rate|0.001|
|`grad_clip`|To prevent gradient explosion in RNNs, restrict the gradient range|1|
"""

batch_size = 64
epochs = 5
embed_dim = 256
hidden_dim = 512
lr = 0.001
grad_clip = 1

"""# Data Batching
- Use `torch.utils.data.Dataset` to create a data generation tool called  `dataset`.
- The, use `torch.utils.data.DataLoader` to randomly sample from the `dataset` and group the samples into batches.
"""

class Dataset(torch.utils.data.Dataset):
    def __init__(self, sequences):
        self.sequences = sequences

    def __len__(self):
        # return the amount of data
        return len(self.sequences)# Write your code here

    def __getitem__(self, index):
        # Extract the input data x and the ground truth y from the data
        x = self.sequences.iloc[index, 0][:-1]# Write your code here
        y = self.sequences.iloc[index, 1][1:]# Write your code here
        return x, y

# collate function, used to build dataloader
def collate_fn(batch):
    batch_x = [torch.tensor(data[0]) for data in batch]
    batch_y = [torch.tensor(data[1]) for data in batch]
    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])
    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])

    # Pad the input sequence
    pad_batch_x = torch.nn.utils.rnn.pad_sequence(batch_x,
                                                  batch_first=True,
                                                  padding_value=char_to_id['<pad>'])

    pad_batch_y = torch.nn.utils.rnn.pad_sequence(batch_y,
                                                  batch_first=True,
                                                  padding_value=char_to_id['<pad>'])

    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens

ds_train = Dataset(df_train[['char_id_list', 'label_id_list']])
ds_eval = Dataset(df_eval[['char_id_list', 'label_id_list']])

print(ds_train)

# Build dataloader of train set and eval set, collate_fn is the collate function
from torch.utils.data import DataLoader

dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
dl_eval = DataLoader(ds_eval, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

print(dl_train)

"""# Model Design

## Execution Flow
1. Convert all characters in the sentence into embeddings.
2. Pass the embeddings through an LSTM sequentially.
3. The output of the LSTM is passed into another LSTM, and additional layers can be added.
4. The output from all time steps of the final LSTM is passed through a Fully Connected layer.
5. The character corresponding to the maximum value across all output dimensions is selected as the next character.

## Loss Function
Since this is a classification task, Cross Entropy is used as the loss function.

## Gradient Update
Adam algorithm is used for gradient updates.
"""

class CharRNN(torch.nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super(CharRNN, self).__init__()

        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,
                                            embedding_dim=embed_dim,
                                            padding_idx=char_to_id['<pad>'])

        self.rnn_layer1 = torch.nn.LSTM(input_size=embed_dim,
                                        hidden_size=hidden_dim,
                                        batch_first=True,)

        self.rnn_layer2 = torch.nn.LSTM(input_size=hidden_dim,
                                        hidden_size=hidden_dim,
                                        batch_first=True,)

        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=hidden_dim,
                                                          out_features=hidden_dim),
                                          torch.nn.ReLU(),
                                          torch.nn.Linear(in_features=hidden_dim,
                                                          out_features=vocab_size))
        #self.dropout = torch.nn.Dropout(0.3)#防止overfitting

    def forward(self, batch_x, batch_x_lens):
        return self.encoder(batch_x, batch_x_lens)

    # The forward pass of the model
    def encoder(self, batch_x, batch_x_lens):
        batch_x = self.embedding(batch_x)

        batch_x = torch.nn.utils.rnn.pack_padded_sequence(batch_x,
                                                          batch_x_lens,
                                                          batch_first=True,
                                                          enforce_sorted=False)

        batch_x, _ = self.rnn_layer1(batch_x)
        batch_x, _ = self.rnn_layer2(batch_x)

        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x,
                                                            batch_first=True)
        # Apply dropout here if needed
        #batch_x = self.dropout(batch_x)#

        batch_x = self.linear(batch_x)


        return batch_x

    def generator(self, start_char, max_len=200):

        char_list = [char_to_id[c] for c in start_char]

        next_char = None
        hidden1 = None  # 第一層 LSTM 的隱藏狀態
        hidden2 = None  # 第二層 LSTM 的隱藏狀態

        while len(char_list) < max_len:
            # Write your code here
            # Pack the char_list to tensor
            input_tensor = torch.tensor(char_list).unsqueeze(0)  # 取最後一個字符並轉換為批次大小為 1 的張量  ##由chatgpt提供寫法##
            input_tensor = self.embedding(input_tensor)  # 通過嵌入層
            # Input the tensor to the embedding layer, LSTM layers, linear respectively

            output, hidden1 = self.rnn_layer1(input_tensor, hidden1)  # 第一層 LSTM
            output, hidden2 = self.rnn_layer2(output, hidden2)  # 第二層 LSTM

            y = self.linear(output[:, -1, :])# Obtain the next token prediction y  ##由chatgpt提供寫法##

            next_char = torch.argmax(y, dim=-1).item()# Use argmax function to get the next token prediction

            if next_char == char_to_id['<eos>']:
                break

            char_list.append(next_char)

        return [id_to_char[ch_id] for ch_id in char_list]

torch.manual_seed(2)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Write your code here. Specify a device (cuda or cpu)

model = CharRNN(vocab_size,
                embed_dim,
                hidden_dim)

criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

"""# Training
1. The outer `for` loop controls the `epoch`
    1. The inner `for` loop uses `data_loader` to retrieve batches.
        1. Pass the batch to the `model` for training.
        2. Compare the predicted results `batch_pred_y` with the true labels `batch_y` using Cross Entropy to calculate the loss `loss`
        3. Use `loss.backward` to automatically compute the gradients.
        4. Use `torch.nn.utils.clip_grad_value_` to limit the gradient values between `-grad_clip` &lt; and &lt; `grad_clip`.
        5. Use `optimizer.step()` to update the model (backpropagation).
2.  After every `1000` batches, output the current loss to monitor whether it is converging.
"""

print(model)

from tqdm import tqdm
from copy import deepcopy
model = model.to(device)
model.train()
i = 0
for epoch in range(1, epochs+1):
    # The process bar
    bar = tqdm(dl_train, desc=f"Train epoch {epoch}")
    for batch_x, batch_y, batch_x_lens, batch_y_lens in bar:
        # Write your code here
        # Clear the gradient
        optimizer.zero_grad()

        batch_pred_y = model(batch_x.to(device), batch_x_lens)

        # Write your code here
        # Input the prediction and ground truths to loss function
        batch_y = batch_y.to(device)  # 將目標張量移到同樣的設備上
        loss = criterion(batch_pred_y.view(-1, vocab_size), batch_y.view(-1))

        # Back propagation
        loss.backward()

        torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip) # gradient clipping

        # Write your code here
        # Optimize parameters in the model
        optimizer.step()

        i+=1
        if i%50==0:
            bar.set_postfix(loss = loss.item())

    # Evaluate your model
    pad_id = char_to_id['<pad>']  # `<pad>` 的 ID
    eos_id = char_to_id['<eos>']  # `<eos>` 的 ID

    bar = tqdm(dl_eval, desc=f"Validation epoch {epoch}")
    matched = 0
    total = 0
    for batch_x, batch_y, batch_x_lens, batch_y_lens in bar:

        predictions = model(batch_x.to(device), batch_x_lens)# Write your code here. Input the batch_x to the model and generate the predictions

        # 使用 argmax 將模型輸出轉換為類別 ID
        predictions = torch.argmax(predictions, dim=-1)

        # Write your code here.
        # Check whether the prediction match the ground truths
        # Compute exact match (EM) on the eval dataset
        # EM = correct/total

        for pred, true in zip(predictions, batch_y):
            # 移動到 CPU 並轉換為列表
            pred = pred.cpu().tolist()
            true = true.cpu().tolist()

            # 找到 true 中第1個非 `<pad>` 的位置
            start_idx = next((i for i, token in enumerate(true) if token != pad_id), len(true))  ##由chatgpt提供寫法##

            # 找到 `true` 中 `<eos>` 的位置，並將其設為結尾索引
            end_idx = next((i for i, token in enumerate(true) if token == eos_id), len(true))+1  ##由chatgpt提供寫法##

            # 比較從非 `<pad>` 開始且在 `<eos>` 之前的部分
            if pred[start_idx:end_idx] == true[start_idx:end_idx]:
                matched += 1
            total += 1



    print(matched/total)

"""# Generation
Use `model.generator` and provide an initial character to automatically generate a sequence.
"""

model = model.to("cpu")
print("".join(model.generator('1+1=')))
print("".join(model.generator('5-6=')))
print("".join(model.generator('3*3=')))
print("".join(model.generator('21+35=')))
print("".join(model.generator('12-14=')))
print("".join(model.generator('8+12=')))
print("".join(model.generator('6-19=')))
print("".join(model.generator('2*27=')))

print("".join(model.generator('123+456=')))

print("".join(model.generator('1+*1=')))
print("".join(model.generator(')5(6+)=')))
print("".join(model.generator('====')))

!pip freeze | grep -E 'numpy|pandas|torch|matplotlib|seaborn|opencc|scikit-learn|tqdm|requests' > requirements.txt