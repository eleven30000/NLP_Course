# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1388d6ECiMcGQdNrf1wZqxfn7kXp9wRWQ

## Part I: Data Pre-processing
"""

import pandas as pd

# Download the Google Analogy dataset
!wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"
with open(f"{file_name}.txt", "r") as f:
    data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:10]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntatic` category.


# 讀取txt檔案
with open('questions-words.txt', 'r') as file:
    data = file.readlines()

# 清理每一行的換行符號
data = [line.strip() for line in data]

# 初始化變量
categories = []
current_category = None
current_subcategory = None

# 計數器來追踪前5個
semantic_count = 0

# 遍歷每一行，分類不同的SubCategory
for line in data:
    if line.startswith(":"):  # 如果以冒號開頭，表示是新的子分類
        current_subcategory = line  # 更新當前的子分類名稱

        # 更新 Category 為 Semantic 或 Syntatic
        if semantic_count < 5:
            current_category = 'Semantic'
            semantic_count += 1
        else:
            current_category = 'Syntatic'
    else:
        # 將數據行和對應的Category、SubCategory加入列表
        categories.append([line, current_category, current_subcategory])

# 創建DataFrame，包含三個列：問題、類別、子分類
df = pd.DataFrame(categories, columns=['Question', 'Category', 'SubCategory'])

# 顯示結果
df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

MODEL_NAME = "glove-wiki-gigaword-100"
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

# Load the pre-trained model (using GloVe vectors here)
model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
      # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      """ Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      """
      # Unpack the analogy: assume format "word_a word_b word_c word_d"
      words = analogy.split()
      if len(words) != 4:
          continue  # skip if the analogy doesn't have exactly four words

      word_a, word_b, word_c, word_d = words

      # Preserve the gold answer (word_d)
      golds.append(word_d)

      # Perform vector arithmetic: word_b + word_c - word_a
      try:
          # Gensim's `most_similar` can solve this analogy problem
          predicted_word = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]
      except KeyError:
          # Handle the case where any of the words are not in the model's vocabulary
          predicted_word = None

      preds.append(predicted_word)

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
filtered_data = data[data["SubCategory"] == SUB_CATEGORY]

# Collect all unique words from the filtered dataset
words = set()
for analogy in filtered_data["Question"]:
    words.update(analogy.split())  # Split and add words to the set

# Remove any words that are not in the model's vocabulary
valid_words = [word for word in words if word in model]

# Extract vectors for the valid words
word_vectors = np.array([model[word] for word in valid_words])

# Perform t-SNE to reduce dimensions to 2D
tsne = TSNE(n_components=2, random_state=0, perplexity=15, n_iter=1000)
word_vecs_2d = tsne.fit_transform(word_vectors)

# Plot the t-SNE results
plt.figure(figsize=(12, 8))
for i, word in enumerate(valid_words):
    plt.scatter(word_vecs_2d[i, 0], word_vecs_2d[i, 1])
    plt.text(word_vecs_2d[i, 0] + 0.02, word_vecs_2d[i, 1] + 0.02, word, fontsize=12)

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.
"""

# Download the split Wikipedia files
# Each file contain 562365 lines (articles).
!gdown --id 1J0os1846PQ129t720aI0wMm-5GepEwSl -O wiki_texts_part_0.txt.gz
!gdown --id 1tsI3RSKPN3b2-1IZ0N7bmjgVRf-THIkW -O wiki_texts_part_1.txt.gz
!gdown --id 1koiw6RFNzDe6pe2zMTfVhsEKmpmnYyu5 -O wiki_texts_part_2.txt.gz
!gdown --id 1YSGbDqhbg2xJsWD_hYQ5z9URl0dCTC2m -O wiki_texts_part_3.txt.gz
!gdown --id 1PA3C99C8CcLFjkenT0a9iU07XEQmXyG_ -O wiki_texts_part_4.txt.gz

# Download the split Wikipedia files
# Each file contain 562365 lines (articles), except the last file.
!gdown --id 1sSLea4hq6Z7oT6noOU_II1ahWjNOKcDX -O wiki_texts_part_5.txt.gz
!gdown --id 1i6kXTDtZkRiivJ0mj-5GkVbE4gMFlmSb -O wiki_texts_part_6.txt.gz
!gdown --id 1ain2DN1nxXfsmJ2Aj9TFZlLVJSPsu9Jb -O wiki_texts_part_7.txt.gz
!gdown --id 1UKhvielQDqQz5pMZ7J3SHv9m8_8gO-dE -O wiki_texts_part_8.txt.gz
!gdown --id 1q1zMA4hbMS7tID2GTQx-c94UPB8YQaaa -O wiki_texts_part_9.txt.gz
!gdown --id 1-kkGxwMxPsoGg5_2pdaOeE3Way6njLpH -O wiki_texts_part_10.txt.gz

# Extract the downloaded wiki_texts_parts files.
!gunzip -k wiki_texts_part_*.gz

import csv

# 讀取 CSV 檔案
csv_file = "world-data-2023.csv"  # 替換成你的CSV檔案名稱
txt_file = "wiki_texts_part_11.txt"  # 輸出TXT檔案名稱

with open(csv_file, 'r', encoding='utf-8') as csvfile:
    csvreader = csv.reader(csvfile)

    # 打開TXT檔案準備寫入
    with open(txt_file, 'w', encoding='utf-8') as txtfile:
        for row in csvreader:
            # 將每一列的數據寫入TXT檔案，並用空格分隔
            txtfile.write(' '.join(row) + '\n')

print("CSV已成功轉換為TXT")

# Combine the extracted wiki_texts_parts files.
!cat wiki_texts_part_*.txt > wiki_texts_combined.txt

# Check the first ten lines of the combined file
!head -n 10 wiki_texts_combined.txt

"""Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random

wiki_txt_path = "wiki_texts_combined.txt"
output_path = "wiki_sampled.txt"  # 輸出文件的路徑

# wiki_texts_combined.txt is a text file separated by linebreaks (\n).
# Each row in wiki_texts_combined.txt indicates a Wikipedia article.

# 設置樣本比例（20%）
sample_ratio = 0.2

sampled_articles = []
line_count = 0
sampled_count = 0

with open(wiki_txt_path, "r", encoding="utf-8") as f:
    # TODO4: Sample `20%` Wikipedia articles
    # Write your code here
    for line in f:
        line_count += 1
        # 生成隨機數，80% 機率跳過，20% 機率讀取該行
        if random.random() < sample_ratio:
            sampled_articles.append(line)
            sampled_count += 1

# 將抽取的文章寫入輸出文件
with open(output_path, "w", encoding="utf-8") as output_file:
    output_file.writelines(sampled_articles)

print(f"Sampled {len(sampled_articles)} articles from {line_count} total lines.")

!unzip wiki_sampled.zip

import gensim
import string
import re
from gensim.models import Word2Vec
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.
# Load the sampled articles
wiki_sampled_path = "wiki_sampled.txt"

# 停用詞表
stop_words = set(stopwords.words('english'))

# 定義流式讀取器來逐行讀取並處理文本
class WikiCorpus:
    def __init__(self, file_path):
        self.file_path = file_path

    def __iter__(self):
        with open(self.file_path, "r", encoding="utf-8") as f:
            for line in f:
                yield self.preprocess(line)

    # 預處理函數：轉小寫、去標點符號、分詞
    def preprocess(self, text):
        # 轉換為小寫
        text = text.lower()
        # 移除標點符號
        text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
        # 分詞
        tokens = text.split()
        # 移除停用詞
        tokens = [word for word in tokens if word not in stop_words]
        return tokens

# 載入和預處理 Wikipedia 文章
wiki_sampled_path = "wiki_sampled.txt"

# 創建流式數據集
wiki_corpus = WikiCorpus(wiki_sampled_path)

# 初始化 Word2Vec 模型
word2vec_model = Word2Vec(
    vector_size=100,   # 嵌入向量維度
    window=5,          # 上下文窗口大小
    min_count=5,       # 忽略出現次數低於 5 次的詞
    workers=4          # 使用 4 個 CPU 核心進行訓練
)

# 構建詞彙表
print("Building vocabulary...")
word2vec_model.build_vocab(wiki_corpus)

# 進行模型訓練，逐步更新模型
print("Training Word2Vec model...")
word2vec_model.train(wiki_corpus, total_examples=word2vec_model.corpus_count, epochs=5)

# 保存模型
word2vec_model.save("wiki_word2vec_stream.model")
print("Word2Vec model trained and saved successfully!")

data = pd.read_csv("questions-words.csv")

from gensim.models import Word2Vec
from tqdm import tqdm

# Load the trained Word2Vec model
model = Word2Vec.load("wiki_word2vec_stream.model")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
      # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      """ Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      """
      # Unpack the analogy: assume format "word_a word_b word_c word_d"
      words = analogy.split()
      if len(words) != 4:
          continue  # skip if the analogy doesn't have exactly four words

      word_a, word_b, word_c, word_d = words

      # Preserve the gold answer (word_d)
      golds.append(word_d)

      try:
          # Perform vector arithmetic: word_b + word_c - word_a
          predicted_word = model.wv.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)[0][0]
      except KeyError:
          # Handle the case where any of the words are not in the model's vocabulary
          predicted_word = None

      preds.append(predicted_word)

# Now `preds` contains the predicted words and `golds` contains the true answers (word_d)

import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
filtered_data = data[data["SubCategory"] == SUB_CATEGORY]# Load the trained Word2Vec model

model = Word2Vec.load("wiki_word2vec_stream.model")

# Collect all unique words from the filtered dataset
words = set()
for analogy in filtered_data["Question"]:
    words.update(analogy.split())  # Split and add words to the set

# Remove any words that are not in the model's vocabulary
valid_words = [word for word in words if word in model.wv]

# Extract vectors for the valid words
word_vectors = np.array([model.wv[word] for word in valid_words])

# Perform t-SNE to reduce dimensions to 2D
tsne = TSNE(n_components=2, random_state=0, perplexity=15, n_iter=1000)
word_vecs_2d = tsne.fit_transform(word_vectors)

# Plot the t-SNE results
plt.figure(figsize=(12, 8))
for i, word in enumerate(valid_words):
    plt.scatter(word_vecs_2d[i, 0], word_vecs_2d[i, 1])
    plt.text(word_vecs_2d[i, 0] + 0.02, word_vecs_2d[i, 1] + 0.02, word, fontsize=12)

plt.title("Word Relationships from Google Analogy Task")
plt.savefig("word_relationships.png", bbox_inches="tight")
plt.show()

!pip freeze | grep -E 'pandas|numpy|gensim|tqdm|matplotlib|scikit-learn|nltk|requests'  > requirements.txt